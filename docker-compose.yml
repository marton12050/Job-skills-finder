version: '3.8'

x-airflow-common:
    &airflow-common
    image: apache/airflow:2.7.1-python3.11
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@localhost:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__LOGGING_LEVEL=INFO
    volumes:
      - ./dags:/opt/airflow/dags
    depends_on:
      - postgres

services:
  kafka: # Kafka is a distributed streaming platform
    image: wurstmeister/kafka
    ports:
      - "9092:9092"
    environment: 
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "your_kafka_topic:1:1"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

  zookeeper: #It is necessary for the kafka metadata(the data about data)   
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"

  spark: # Spark is a distributed data processing engine
    image: bitnami/spark:3
    environment:
      - SPARK_MODE=master
    ports:
      - "4040:4040"
      - "6066:6066"
      - "7077:7077"
      - "8080:8080"

  spark-worker:
    image: bitnami/spark:3
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    depends_on:
      - spark

  mongo:
    image: mongo:latest
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
    volumes:
      - mongo-db:/data/db
      
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=airflow
      - POSTGRES_PORT=5432
    ports:
      - "5432:5432"

  airflow-init:
    << : *airflow-common
    container_name: airflow_init
    entrypoint: /bin/bash
    command:
      - -c
      - airflow users list || ( airflow db init &&
        airflow users create
          --role Admin
          --username airflow
          --password airflow
          --email airflow@airflow.com
          --firstname airflow
          --lastname airflow )
    restart: on-failure

  airflow-webserver:
    << : *airflow-common
    command: airflow webserver
    ports:
      - 8081:8080
    container_name: airflow_webserver
    restart: always

  airflow-scheduler:
    << : *airflow-common
    command: airflow scheduler
    container_name: airflow_scheduler
    restart: always

volumes: # So that the databases are not lost when the containers is restarted
  mongo-db:
